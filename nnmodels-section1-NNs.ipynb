{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2cnpmGXiNaD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "import torch.utils.data as d\n",
    "import tokenization_dim_reduction as tdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'D:\\Researching Data\\Youtube data\\USvideos.csv'\n",
    "torch.backends.cudnn.deterministic = True\n",
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dtext, dlabel = tdr.select_col(data_dir, tdr.cols_t4)\n",
    "new_TEXT = tdr.combine_text(dtext, 1, [0,2])\n",
    "#new_label = tdr.multi_to_binary(dlabel, 25) # politics\n",
    "new_label = tdr.multi_to_binary(dlabel, 24) # entertainments\n",
    "new_arr = np.concatenate((new_TEXT.reshape([len(new_TEXT),1]), new_label), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.492048496299795  percent of videos are labelled as the selected category\n",
      "the baseline precision is  25.492048496299795  in this model\n"
     ]
    }
   ],
   "source": [
    "print((new_label[new_label == 1].shape[0] / new_label.shape[0]) * 100,\n",
    "      \" percent of videos are labelled as the selected category\")\n",
    "print(\"the baseline precision is \", \n",
    "      (new_label[new_label == 1].shape[0] / new_label.shape[0]) * 100,\n",
    "     \" in this model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ml75oPv6iNaJ"
   },
   "outputs": [],
   "source": [
    "# split train, validation, test\n",
    "def split_train_test(dt_size, train_valid_test_r):\n",
    "    '''\n",
    "    The function randomly selects the indices for\n",
    "    training, validation, and testing sets\n",
    "    Inputs:\n",
    "        dt_size: number of rows\n",
    "        train_valid_test_r: tuple of ratios\n",
    "    Return: indices for each subset\n",
    "    '''\n",
    "    train_size = int(dt_size * train_valid_test_r[0] // 1)\n",
    "    valid_size = int(dt_size * train_valid_test_r[1] // 1)\n",
    "    test_size = int(dt_size - train_size - valid_size)\n",
    "    print(\"The size of train, valid and test data are\", train_size, valid_size, test_size)\n",
    "    \n",
    "    full_indices = np.arange(0, dt_size, 1)\n",
    "    train_indices = np.random.permutation(full_indices)[:train_size]\n",
    "    \n",
    "    sub_indices = set(full_indices) - set(train_indices)\n",
    "    valid_indices = np.random.permutation(list(sub_indices))[:valid_size]\n",
    "    \n",
    "    sub_indicest = set(sub_indices) - set(valid_indices)\n",
    "    test_indices = np.array(list(sub_indicest))\n",
    "    \n",
    "    return train_indices, valid_indices, test_indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(path, arr, train_valid_test_r=(0.4, 0.4, 0.2)):\n",
    "    '''\n",
    "    The function split the data to train, validation and test\n",
    "    sets with randomly selected indices and save them to seperated\n",
    "    csv files\n",
    "    Inputs:\n",
    "        path: directory of the saved files\n",
    "        arr: the whole dataset\n",
    "        train_valid_test_r: tuple of ratios\n",
    "    '''\n",
    "    train_indices, valid_indices, test_indices = split_train_test(arr.shape[0], train_valid_test_r)\n",
    "    pd.DataFrame(arr[train_indices]).to_csv(path + \"\\\\train.csv\", header=None, index=None)\n",
    "    pd.DataFrame(arr[valid_indices]).to_csv(path + \"\\\\valid.csv\", header=None, index=None)\n",
    "    pd.DataFrame(arr[test_indices]).to_csv(path + \"\\\\test.csv\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of train, valid and test data are 2540 2540 1271\n",
      "Number of training examples: 2539\n",
      "Number of testing examples: 1270\n",
      "Number of validation examples:1270\n"
     ]
    }
   ],
   "source": [
    "path = r'D:\\Researching Data\\Youtube data'\n",
    "split_data(path, new_arr, train_valid_test_r=(0.4, 0.4, 0.2))\n",
    "fields = [(\"text\", TEXT), (\"label\", LABEL)]\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "                                        path = path,\n",
    "                                        train = 'train.csv',\n",
    "                                        validation = 'valid.csv',\n",
    "                                        test = 'test.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = fields,\n",
    "                                        skip_header = True)\n",
    "MAX_VOCAB_SIZE = 25000\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "print(f'Number of validation examples:{len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wn-__tIFiNak"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data), \n",
    "         batch_size = BATCH_SIZE,\n",
    "         sort_key = lambda x: len(x.text),\n",
    "         sort_within_batch = True, \n",
    "         device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is inspired by and modified from the PyTorch Tutorial of Ben Trevett, and assignment code of CAPP 30255, part of the modification will be marked with comments\n",
    "################################################################################### <br>\n",
    "First Source: <br>\n",
    "Topic: Tutorials on getting started with PyTorch and TorchText for sentiment analysis <br>\n",
    "Source: https://github.com/bentrevett/pytorch-sentiment-analysis <br>\n",
    "Author: Ben Trevett <br>\n",
    "Date: 2019 <br>\n",
    "\n",
    "Second Source: <br>\n",
    "Topic: Assignment 2 of CAPP 30255, The University of Chicago <br>\n",
    "Author: Amitabh Chaudhary <br>\n",
    "Date: 2020 <br>\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C6thdhypiNao"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Return accuracy per batch\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_precision(preds, y):\n",
    "    '''\n",
    "    Return precision per batch\n",
    "    '''\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    prec_correct = ((rounded_preds == y) & (rounded_preds == 1)).float()\n",
    "    prec_total = (rounded_preds == 1).float()\n",
    "    precision = prec_correct.sum() / prec_total.sum()\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_recall(preds, y):\n",
    "    '''\n",
    "    Return recall per batch\n",
    "    '''\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    rec_correct = ((rounded_preds == y) & (rounded_preds == 1)).float()\n",
    "    rec_total = (y == 1).float()\n",
    "    recall = rec_correct.sum() / rec_total.sum()\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.RNN = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)  \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #print(\"simpleRNN text:, \", text.size())\n",
    "        emb = self.embedding(text)\n",
    "        ot1, hidden = self.RNN(emb)\n",
    "        ot2 = self.relu(hidden.squeeze(0))\n",
    "        output = self.linear(ot2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers, bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.RNN = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, text):\n",
    "        \n",
    "        # Simplify the original version of LSTM implementation\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        output, (hidden, cell) = self.RNN(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "            \n",
    "        return self.linear(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "                \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "                \n",
    "        embedded = self.embedding(torch.transpose(text, 0, 1))\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GWyqWHbTiNat"
   },
   "outputs": [],
   "source": [
    "class WordEmbAvg_2linear(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)  \n",
    "        self.relu = nn.ReLU()\n",
    "                                 \n",
    "    def forward(self, text):\n",
    "        \n",
    "        # Modify the original version of the CAPP 30255 assignment\n",
    "        emb = self.embedding(text)\n",
    "        emb = torch.mean(emb, dim=0).squeeze(1)\n",
    "        ot1 = self.linear1(emb)\n",
    "        ot2 = self.relu(ot1)\n",
    "        output = self.linear2(ot2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YD1Y0axriNa3"
   },
   "outputs": [],
   "source": [
    "class Training_module( ):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        \n",
    "        self.model = model\n",
    "        self.loss_fn = (nn.BCEWithLogitsLoss()).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.01)\n",
    "    \n",
    "    def train_epoch(self, iterator):\n",
    "        '''\n",
    "        Train the model for one epoch. For this repeat the following, \n",
    "        going through all training examples.\n",
    "        1. Get the next batch of inputs from the iterator.\n",
    "        2. Determine the predictions using a forward pass.\n",
    "        3. Compute the loss.\n",
    "        4. Compute gradients using a backward pass.\n",
    "        5. Execute one step of the optimizer to update the model paramters.\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            predictions = self.model(batch.text).squeeze(1)\n",
    "            loss = self.loss_fn(predictions, batch.label)\n",
    "            accuracy = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy.item()\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def train_model(self, train_iterator, dev_iterator):\n",
    "        \"\"\"\n",
    "        Train the model for multiple epochs, and after each evaluate on the\n",
    "        development set.  Return the best performing model.\n",
    "        \"\"\"  \n",
    "        dev_accs = [0.]\n",
    "        for epoch in range(5):\n",
    "            self.train_epoch(train_iterator)\n",
    "            dev_acc = self.evaluate(dev_iterator)\n",
    "            print(f\"Epoch {epoch}: Dev Accuracy: {dev_acc[1]} Dev Loss:{dev_acc[0]}\")\n",
    "            if dev_acc[1] > max(dev_accs):\n",
    "                best_model = copy.deepcopy(self)\n",
    "            dev_accs.append(dev_acc[1])\n",
    "        return best_model.model\n",
    "                \n",
    "    def evaluate(self, iterator):\n",
    "        '''\n",
    "        Evaluate the performance of the model on the given examples.\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        epoch_prec = 0\n",
    "        epoch_rec = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "    \n",
    "            for batch in iterator:\n",
    "\n",
    "                predictions = self.model(batch.text).squeeze(1)\n",
    "                loss = self.loss_fn(predictions, batch.label)\n",
    "                acc = binary_accuracy(predictions, batch.label)\n",
    "                precision = binary_precision(predictions, batch.label)\n",
    "                recall = binary_recall(predictions, batch.label)\n",
    "        \n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "                epoch_prec += precision.item()\n",
    "                epoch_rec += recall.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator), \\\n",
    "               epoch_prec / len(iterator), epoch_rec / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "\n",
    "model_wordem = WordEmbAvg_2linear(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX)\n",
    "model_rnn = SimpleRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX)\n",
    "model_BLSTM = LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, \n",
    "                   N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "model_CNN = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "MODEL_DICT = {\"avg_embedding\": model_wordem, \"SimpleRNN\": model_rnn, \"BLSTM\": model_BLSTM, \"CNN\": model_CNN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(model_dict, model_txt=\"avg_embedding\"):\n",
    "    '''\n",
    "    Helper function for model selection\n",
    "    '''\n",
    "    return model_dict[model_txt]\n",
    "\n",
    "# Comparing the performance of different models, the original \n",
    "# version of the CAPP 30255 assignment\n",
    "def compare_models(model_dict):\n",
    "    '''\n",
    "    The function presents and compare the performances of\n",
    "    different neural network models and store the best\n",
    "    models of each model type in the output dictionary\n",
    "    \n",
    "    Inputs: model_dict: dictionary of model types used in training\n",
    "    Return: dictionary of best models of each model type\n",
    "    '''\n",
    "    best_models = {}\n",
    "    for key, value in model_dict.items():\n",
    "        print(\"currently training the model: \", key)\n",
    "\n",
    "        model = model_selection(model_dict, key)\n",
    "        model = model.to(device)\n",
    "        tm = Training_module(model)\n",
    "        best_model = tm.train_model(train_iterator, valid_iterator)\n",
    "        best_models[key] = best_model\n",
    "        \n",
    "        tm.model = best_model\n",
    "        test_loss, test_acc, test_prec, test_rec = tm.evaluate(test_iterator)\n",
    "        print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "        print(f'Test Prec: {test_prec*100:.3f}% | Test Rec: {test_rec*100:.3f}%')\n",
    "        \n",
    "    return best_models\n",
    "\n",
    "# Searching for phrases with highest norm values, modify the original \n",
    "# version of the CAPP 30255 assignment\n",
    "def get_effective_norms(best_models, selected_mkey=\"avg_embedding\"):\n",
    "    '''\n",
    "    The model presents 10 most effective and 10 less effective\n",
    "    phrases used in the classification\n",
    "    Inputs: \n",
    "        best_models: dictionary of best model of each model type\n",
    "        selected_mkey: selected model type\n",
    "    '''\n",
    "    best_model = best_models[selected_mkey]\n",
    "    strong_words = []\n",
    "    weak_words = []\n",
    "    emb_weight = best_model.embedding.weight.data\n",
    "    top_indices = torch.norm(emb_weight, p=2, dim=1).detach().topk(10).indices\n",
    "    bottom_indices = torch.norm(emb_weight, p=2, dim=1).detach().topk(10, largest=False).indices\n",
    "\n",
    "    for idx in top_indices:\n",
    "        strong_words.append(TEXT.vocab.itos[idx])\n",
    "    \n",
    "    for idx in bottom_indices:\n",
    "        weak_words.append(TEXT.vocab.itos[idx])\n",
    "    \n",
    "    print(\"most effective words: \", strong_words)\n",
    "    print(\"less effective words: \", weak_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently training the model:  avg_embedding\n",
      "Epoch 0: Dev Accuracy: 0.7498637348413467 Dev Loss:0.5612356200814247\n",
      "Epoch 1: Dev Accuracy: 0.8765897527337074 Dev Loss:0.3659035500138998\n",
      "Epoch 2: Dev Accuracy: 0.8795058146119118 Dev Loss:0.3773972377181053\n",
      "Epoch 3: Dev Accuracy: 0.8787336483597755 Dev Loss:0.46051947474479676\n",
      "Epoch 4: Dev Accuracy: 0.8769712939858436 Dev Loss:0.4851145945489407\n",
      "Test Loss: 0.402 | Test Acc: 86.63%\n",
      "Test Prec: 76.760% | Test Rec: 64.411%\n",
      "currently training the model:  SimpleRNN\n",
      "Epoch 0: Dev Accuracy: 0.7467387348413468 Dev Loss:0.5705512657761573\n",
      "Epoch 1: Dev Accuracy: 0.7416606098413467 Dev Loss:0.5794503778219223\n",
      "Epoch 2: Dev Accuracy: 0.7467387348413468 Dev Loss:0.5686986550688744\n",
      "Epoch 3: Dev Accuracy: 0.7467387348413468 Dev Loss:0.5735377579927444\n",
      "Epoch 4: Dev Accuracy: 0.7400981098413467 Dev Loss:0.5935619696974754\n",
      "Test Loss: 0.585 | Test Acc: 73.53%\n",
      "Test Prec: nan% | Test Rec: 0.000%\n",
      "currently training the model:  BLSTM\n",
      "Epoch 0: Dev Accuracy: 0.7498637348413467 Dev Loss:0.5573898144066334\n",
      "Epoch 1: Dev Accuracy: 0.7746638804674149 Dev Loss:0.5219143912196159\n",
      "Epoch 2: Dev Accuracy: 0.7613917157053948 Dev Loss:0.7326723709702492\n",
      "Epoch 3: Dev Accuracy: 0.7914607554674149 Dev Loss:0.5635159157216549\n",
      "Epoch 4: Dev Accuracy: 0.8004451304674148 Dev Loss:0.6012772344052791\n",
      "Test Loss: 0.579 | Test Acc: 80.70%\n",
      "Test Prec: 67.850% | Test Rec: 48.036%\n",
      "currently training the model:  CNN\n",
      "Epoch 0: Dev Accuracy: 0.754151526093483 Dev Loss:0.5309718675911427\n",
      "Epoch 1: Dev Accuracy: 0.8107739821076393 Dev Loss:0.47408592849969866\n",
      "Epoch 2: Dev Accuracy: 0.8361736923456192 Dev Loss:0.6748409010469913\n",
      "Epoch 3: Dev Accuracy: 0.8392986923456192 Dev Loss:0.6780470363795758\n",
      "Epoch 4: Dev Accuracy: 0.8605832114815712 Dev Loss:1.0493452064692974\n",
      "Test Loss: 1.099 | Test Acc: 85.12%\n",
      "Test Prec: 74.506% | Test Rec: 62.078%\n",
      "most effective words:  ['caught', 'sadistic', 'intersect', 'Paradoxes', 'Rafe', 'copter', 'authentic', 'Westbrook', 'Powder', 'butterbeer']\n",
      "less effective words:  ['<pad>', 'managing', 'SXyObZahu', 'privileges', 'Sock', 'antetokounmpo', '⇢', 'recap', 'Buys', 'http://www.businessinsider.com/\\\\n\\\\n--------------------------------------------------\\\\n\\\\nBusiness']\n"
     ]
    }
   ],
   "source": [
    "best_models = compare_models(MODEL_DICT)\n",
    "get_effective_norms(best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "KnhR3qF_iNYc"
   ],
   "name": "mlpp20-hw2-collab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
