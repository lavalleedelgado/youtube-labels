{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2cnpmGXiNaD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "import torch.utils.data as d\n",
    "import tokenization_dim_reduction as tdr\n",
    "import ngrams as ng\n",
    "\n",
    "data_dir = r'D:\\Researching Data\\Youtube data\\USvideos.csv'\n",
    "torch.backends.cudnn.deterministic = True\n",
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dtext, dlabel = tdr.select_col(data_dir, tdr.cols_t4)\n",
    "new_TEXT = tdr.combine_text(dtext, 1, [0,2])\n",
    "#new_label = tdr.multi_to_binary(dlabel, 25) # politics\n",
    "new_label = tdr.multi_to_binary(dlabel, 24) # entertainments\n",
    "new_arr = np.concatenate((new_TEXT.reshape([len(new_TEXT),1]), new_label), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_creater(n, sentence, whole_grams):\n",
    "    '''\n",
    "    The function creates n-grams dictionary with its count\n",
    "    Input:\n",
    "        n: n for n-grams\n",
    "        sentence: single sentence passed in\n",
    "        whole_grams: dictionary with accumulated count\n",
    "    Returns:\n",
    "        grams: n-grams with its counts for the sentence\n",
    "        updated whole_grams\n",
    "    '''\n",
    "    grams = {}\n",
    "    word_lst = ng.clean_punctuation(sentence).split()\n",
    "    for i in range(len(word_lst) - n + 1):\n",
    "        gram = \" \".join(word_lst[i:i + n])\n",
    "        if not grams.get(gram):\n",
    "            grams[gram] = 0\n",
    "            whole_grams[gram] = 0\n",
    "        grams[gram] += 1\n",
    "        whole_grams[gram] += 1\n",
    "    \n",
    "    return grams, whole_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with n-grams, used in simple machine learning models\n",
    "def word_ngrams(n, txt_arr):\n",
    "    '''\n",
    "    The function creates the ngrams with its corresponding\n",
    "    counts for the whole dataset\n",
    "    Inputs:\n",
    "        n: n for n-grams\n",
    "        txt_arr: all array with train, valid and test sets\n",
    "    Returns:\n",
    "        d-grams: dictionary of ngrams and counts of each row\n",
    "        whole_grams: dictionary with accumulated count\n",
    "    '''\n",
    "    whole_grams = {}\n",
    "    d_grams = {}\n",
    "    for didx, txt in enumerate(txt_arr):\n",
    "        grams, whole_grams = ngram_creater(n, txt, whole_grams)\n",
    "        d_grams[didx] = grams\n",
    "\n",
    "    return d_grams, whole_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_index(n, txt_arr):\n",
    "    '''\n",
    "    The function assigns index to each unique words\n",
    "    or n-grams.\n",
    "    Inputs:\n",
    "        n: n for n-grams\n",
    "        txt_arr: all array with train, valid and test sets\n",
    "    Returns:\n",
    "        word_to_idx: dictionary mapping word to index\n",
    "        wtorch: dictionary mapping word to counts\n",
    "    '''\n",
    "    count = 0\n",
    "    word_to_idx = {}\n",
    "    _, whole_grams = word_ngrams(n, txt_arr)\n",
    "    wtorch = torch.zeros(len(whole_grams))\n",
    "    \n",
    "    for ngrams, ct in whole_grams.items():\n",
    "        word_to_idx[ngrams] = count\n",
    "        wtorch[count] = ct\n",
    "        count += 1\n",
    "    \n",
    "    return word_to_idx, wtorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_torch(grams, word_to_idx, wtorch):\n",
    "    '''\n",
    "    The function transfers a list of phrase to an\n",
    "    embedding vector.\n",
    "    Inputs:\n",
    "        grams: list of splitted sentence\n",
    "        word_to_idx: dictionary mapping word to index\n",
    "        wtorch: dictionary mapping word to counts\n",
    "    Return: an embedding vector \n",
    "    '''\n",
    "    bow_vec = torch.zeros(len(word_to_idx))\n",
    "    for gram in grams:\n",
    "        bow_vec[word_to_idx[gram]] = wtorch[word_to_idx[gram]]\n",
    "        \n",
    "    return bow_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ml75oPv6iNaJ"
   },
   "outputs": [],
   "source": [
    "# split train, validation, test\n",
    "def split_train_test(dt_size, train_valid_test_r):\n",
    "    '''\n",
    "    The function randomly selects the indices for\n",
    "    training, validation, and testing sets\n",
    "    Inputs:\n",
    "        dt_size: number of rows\n",
    "        train_valid_test_r: tuple of ratios\n",
    "    Return: indices for each subset\n",
    "    '''\n",
    "    train_size = int(dt_size * train_valid_test_r[0] // 1)\n",
    "    valid_size = int(dt_size * train_valid_test_r[1] // 1)\n",
    "    test_size = int(dt_size - train_size - valid_size)\n",
    "    print(\"the size of train, valid and test data are\", train_size, valid_size, test_size)\n",
    "    \n",
    "    full_indices = np.arange(0, dt_size, 1)\n",
    "    train_indices = np.random.permutation(full_indices)[:train_size]\n",
    "    \n",
    "    sub_indices = set(full_indices) - set(train_indices)\n",
    "    valid_indices = np.random.permutation(list(sub_indices))[:valid_size]\n",
    "    \n",
    "    sub_indicest = set(sub_indices) - set(valid_indices)\n",
    "    test_indices = np.array(list(sub_indicest))\n",
    "    \n",
    "    return train_indices, valid_indices, test_indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of train, valid and test data are 2540 2540 1271\n"
     ]
    }
   ],
   "source": [
    "train_indices, valid_indices, test_indices = split_train_test(new_arr.shape[0], (0.4, 0.4, 0.2))\n",
    "X_train = new_TEXT[train_indices]\n",
    "y_train = new_label[train_indices]\n",
    "X_valid = new_TEXT[valid_indices]\n",
    "y_valid = new_label[valid_indices]\n",
    "X_test = new_TEXT[test_indices]\n",
    "y_test = new_label[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx, wtorch = word_to_index(2, new_TEXT)\n",
    "dgrams = word_ngrams(2, X_train)[0]\n",
    "VOCAB_SIZE = len(word_to_idx)\n",
    "NUM_LABELS = 2\n",
    "n = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is inspired by and modified from the PyTorch Tutorial of Robert Guthrie, part of the modification will be marked with comments.\n",
    "####################################################################################### <br>\n",
    "Topic: DEEP LEARNING WITH PYTORCH <br>\n",
    "Author: Robert Guthrie <br>\n",
    "Source: https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#sphx-glr-beginner-nlp-deep-learning-tutorial-py <br>\n",
    "Date: 2017 <br>\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ix = {0.0: 0, 1.0: 1}\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "\n",
    "        super(BoWNN, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        \n",
    "        return self.linear(bow_vec)\n",
    "    \n",
    "model = BoWNN(VOCAB_SIZE, NUM_LABELS)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch number  0 , the accuracy for validation set is  0.8484251968503937\n",
      "For epoch number  1 , the accuracy for validation set is  0.8842519685039371\n",
      "For epoch number  2 , the accuracy for validation set is  0.8818897637795275\n",
      "For epoch number  3 , the accuracy for validation set is  0.8842519685039371\n",
      "For epoch number  4 , the accuracy for validation set is  0.8818897637795275\n",
      "the accuracy for test set is  0.8662470495672698\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for epoch in range(5):\n",
    "    \n",
    "    for idx in range(len(train_indices)):\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        sentence = X_train[idx]\n",
    "        grams, _ = ngram_creater(n, sentence, {})\n",
    "        bow_vec = sentence_torch(grams, word_to_idx, wtorch)\n",
    "        \n",
    "        pred = model(bow_vec.view(1,-1))\n",
    "        loss = loss_function(pred, make_target(y_train[idx, 0], label_to_ix))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    acc_count = 0\n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(valid_indices)):\n",
    "            sentence = X_valid[idx]\n",
    "            grams, _ = ngram_creater(n, sentence, {})\n",
    "            bow_vec = sentence_torch(grams, word_to_idx, wtorch)\n",
    "            pred = model(bow_vec.view(1,-1))\n",
    "\n",
    "            y_pred = np.argmax(pred[0].detach().numpy())\n",
    "            if y_valid[idx, 0] == y_pred:\n",
    "                acc_count += 1\n",
    "\n",
    "    print(\"For epoch number \", epoch, \", the accuracy for validation set is \", \n",
    "          acc_count / len(valid_indices))\n",
    "    \n",
    "    if (acc_count / len(valid_indices)) > best_acc:\n",
    "        best_model = model\n",
    "\n",
    "acc_count = 0\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(test_indices)):\n",
    "        sentence = X_test[idx]\n",
    "        grams, _ = ngram_creater(n, sentence, {})\n",
    "        bow_vec = sentence_torch(grams, word_to_idx, wtorch)\n",
    "        pred = best_model(bow_vec.view(1,-1))\n",
    "        y_pred = np.argmax(pred[0].detach().numpy())\n",
    "        if y_test[idx, 0] == y_pred:\n",
    "            acc_count += 1\n",
    "\n",
    "print(\"the accuracy for test set is \", acc_count / len(test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "KnhR3qF_iNYc"
   ],
   "name": "mlpp20-hw2-collab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
