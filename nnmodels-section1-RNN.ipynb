{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2cnpmGXiNaD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "import torch.utils.data as d\n",
    "import tokenization_dim_reduction as tdr\n",
    "\n",
    "data_dir = r'D:\\Researching Data\\Youtube data\\USvideos.csv'\n",
    "torch.backends.cudnn.deterministic = True\n",
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dtext, dlabel = tdr.select_col(data_dir, tdr.cols_t4)\n",
    "new_TEXT = tdr.combine_text(dtext, 1, [0,2])\n",
    "#new_label = tdr.multi_to_binary(dlabel, 25) # politics\n",
    "new_label = tdr.multi_to_binary(dlabel, 24) # entertainments\n",
    "new_arr = np.concatenate((new_TEXT.reshape([len(new_TEXT),1]), new_label), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.492048496299795  percent of videos are labelled as the selected category\n"
     ]
    }
   ],
   "source": [
    "print((new_label[new_label == 1].shape[0] / new_label.shape[0]) * 100,\n",
    "      \" percent of videos are labelled as the selected category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ml75oPv6iNaJ"
   },
   "outputs": [],
   "source": [
    "# split train, validation, test\n",
    "def split_train_test(dt_size, train_valid_test_r):\n",
    "    '''\n",
    "    The function randomly selects the indices for\n",
    "    training, validation, and testing sets\n",
    "    Inputs:\n",
    "        dt_size: number of rows\n",
    "        train_valid_test_r: tuple of ratios\n",
    "    Return: indices for each subset\n",
    "    '''\n",
    "    train_size = int(dt_size * train_valid_test_r[0] // 1)\n",
    "    valid_size = int(dt_size * train_valid_test_r[1] // 1)\n",
    "    test_size = int(dt_size - train_size - valid_size)\n",
    "    print(\"The size of train, valid and test data are\", train_size, valid_size, test_size)\n",
    "    \n",
    "    full_indices = np.arange(0, dt_size, 1)\n",
    "    train_indices = np.random.permutation(full_indices)[:train_size]\n",
    "    \n",
    "    sub_indices = set(full_indices) - set(train_indices)\n",
    "    valid_indices = np.random.permutation(list(sub_indices))[:valid_size]\n",
    "    \n",
    "    sub_indicest = set(sub_indices) - set(valid_indices)\n",
    "    test_indices = np.array(list(sub_indicest))\n",
    "    \n",
    "    return train_indices, valid_indices, test_indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(path, arr, train_valid_test_r=(0.4, 0.4, 0.2)):\n",
    "    '''\n",
    "    The function split the data to train, validation and test\n",
    "    sets with randomly selected indices and save them to seperated\n",
    "    csv files\n",
    "    Inputs:\n",
    "        path: directory of the saved files\n",
    "        arr: the whole dataset\n",
    "        train_valid_test_r: tuple of ratios\n",
    "    '''\n",
    "    train_indices, valid_indices, test_indices = split_train_test(arr.shape[0], train_valid_test_r)\n",
    "    pd.DataFrame(arr[train_indices]).to_csv(path + \"\\\\train.csv\", header=None, index=None)\n",
    "    pd.DataFrame(arr[valid_indices]).to_csv(path + \"\\\\valid.csv\", header=None, index=None)\n",
    "    pd.DataFrame(arr[test_indices]).to_csv(path + \"\\\\test.csv\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of train, valid and test data are 2540 2540 1271\n",
      "Number of training examples: 2539\n",
      "Number of testing examples: 1270\n",
      "Number of validation examples:1270\n"
     ]
    }
   ],
   "source": [
    "path = r'D:\\Researching Data\\Youtube data'\n",
    "split_data(path, new_arr, train_valid_test_r=(0.4, 0.4, 0.2))\n",
    "fields = [(\"text\", TEXT), (\"label\", LABEL)]\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "                                        path = path,\n",
    "                                        train = 'train.csv',\n",
    "                                        validation = 'valid.csv',\n",
    "                                        test = 'test.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = fields,\n",
    "                                        skip_header = True)\n",
    "MAX_VOCAB_SIZE = 25000\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "print(f'Number of validation examples:{len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wn-__tIFiNak"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data), \n",
    "         batch_size = BATCH_SIZE,\n",
    "         sort_key = lambda x: len(x.text),\n",
    "         sort_within_batch = True, \n",
    "         device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is inspired by and modified from the PyTorch Tutorial of Ben Trevett, and assignment code of CAPP 30255, part of the modification will be marked with comments\n",
    "################################################################################### <br>\n",
    "First Source: <br>\n",
    "Topic: Tutorials on getting started with PyTorch and TorchText for sentiment analysis <br>\n",
    "Source: https://github.com/bentrevett/pytorch-sentiment-analysis <br>\n",
    "Author: Ben Trevett <br>\n",
    "Date: 2019 <br>\n",
    "\n",
    "Second Source: <br>\n",
    "Topic: Assignment 2 of CAPP 30255, The University of Chicago <br>\n",
    "Author: Amitabh Chaudhary <br>\n",
    "Date: 2020 <br>\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C6thdhypiNao"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Return accuracy per batch\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.RNN = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)  \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        emb = self.embedding(text)\n",
    "        ot1, hidden = self.RNN(emb)\n",
    "        ot2 = self.relu(hidden.squeeze(0))\n",
    "        output = self.linear(ot2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers, bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.RNN = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, text):\n",
    "        \n",
    "        # Simplify the original version of LSTM implementation\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        output, (hidden, cell) = self.RNN(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "            \n",
    "        return self.linear(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GWyqWHbTiNat"
   },
   "outputs": [],
   "source": [
    "class WordEmbAvg_2linear(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)  \n",
    "        self.relu = nn.ReLU()\n",
    "                                 \n",
    "    def forward(self, text):\n",
    "        \n",
    "        # Modify the original version of the CAPP 30255 assignment\n",
    "        emb = self.embedding(text)\n",
    "        emb = torch.mean(emb, dim=0).squeeze(1)\n",
    "        ot1 = self.linear1(emb)\n",
    "        ot2 = self.relu(ot1)\n",
    "        output = self.linear2(ot2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YD1Y0axriNa3"
   },
   "outputs": [],
   "source": [
    "class Training_module( ):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        \n",
    "        self.model = model\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.01)\n",
    "    \n",
    "    def train_epoch(self, iterator):\n",
    "        '''\n",
    "        Train the model for one epoch. For this repeat the following, \n",
    "        going through all training examples.\n",
    "        1. Get the next batch of inputs from the iterator.\n",
    "        2. Determine the predictions using a forward pass.\n",
    "        3. Compute the loss.\n",
    "        4. Compute gradients using a backward pass.\n",
    "        5. Execute one step of the optimizer to update the model paramters.\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            predictions = self.model(batch.text).squeeze(1)\n",
    "            loss = self.loss_fn(predictions, batch.label)\n",
    "            accuracy = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += accuracy.item()\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def train_model(self, train_iterator, dev_iterator):\n",
    "        \"\"\"\n",
    "        Train the model for multiple epochs, and after each evaluate on the\n",
    "        development set.  Return the best performing model.\n",
    "        \"\"\"  \n",
    "        dev_accs = [0.]\n",
    "        for epoch in range(5):\n",
    "            self.train_epoch(train_iterator)\n",
    "            dev_acc = self.evaluate(dev_iterator)\n",
    "            print(f\"Epoch {epoch}: Dev Accuracy: {dev_acc[1]} Dev Loss:{dev_acc[0]}\")\n",
    "            if dev_acc[1] > max(dev_accs):\n",
    "                best_model = copy.deepcopy(self)\n",
    "            dev_accs.append(dev_acc[1])\n",
    "        return best_model.model\n",
    "                \n",
    "    def evaluate(self, iterator):\n",
    "        '''\n",
    "        Evaluate the performance of the model on the given examples.\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "    \n",
    "            for batch in iterator:\n",
    "\n",
    "                predictions = self.model(batch.text).squeeze(1)\n",
    "                loss = self.loss_fn(predictions, batch.label)\n",
    "                acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model_wordem = WordEmbAvg_2linear(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX)\n",
    "model_rnn = SimpleRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX)\n",
    "model_BLSTM = LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, \n",
    "                   N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "MODEL_DICT = {\"avg_embedding\": model_wordem, \"SimpleRNN\": model_rnn, \"BLSTM\": model_BLSTM}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently training the model:  avg_embedding\n",
      "Epoch 0: Dev Accuracy: 0.7873364821076393 Dev Loss:0.4706132486462593\n",
      "Epoch 1: Dev Accuracy: 0.8652434587478638 Dev Loss:0.362857586145401\n",
      "Epoch 2: Dev Accuracy: 0.8707122087478638 Dev Loss:0.4724673431366682\n",
      "Epoch 3: Dev Accuracy: 0.8734465837478638 Dev Loss:0.49085457511246205\n",
      "Epoch 4: Dev Accuracy: 0.8738372087478637 Dev Loss:0.5455993581563234\n",
      "Test Loss: 0.516 | Test Acc: 87.66%\n",
      "currently training the model:  SimpleRNN\n",
      "Epoch 0: Dev Accuracy: 0.7395076304674149 Dev Loss:0.5743001639842987\n",
      "Epoch 1: Dev Accuracy: 0.7414607554674149 Dev Loss:0.5727346129715443\n",
      "Epoch 2: Dev Accuracy: 0.7305232554674148 Dev Loss:0.6354811459779739\n",
      "Epoch 3: Dev Accuracy: 0.7305232554674148 Dev Loss:0.5823904797434807\n",
      "Epoch 4: Dev Accuracy: 0.7305232554674148 Dev Loss:0.6065546780824661\n",
      "Test Loss: 0.544 | Test Acc: 76.61%\n",
      "currently training the model:  BLSTM\n",
      "Epoch 0: Dev Accuracy: 0.7356013804674149 Dev Loss:0.5658457770943641\n",
      "Epoch 1: Dev Accuracy: 0.7508357554674149 Dev Loss:0.5432783268392086\n",
      "Epoch 2: Dev Accuracy: 0.7522074848413467 Dev Loss:0.5765933513641357\n",
      "Epoch 3: Dev Accuracy: 0.7785519614815712 Dev Loss:0.5575529113411903\n",
      "Epoch 4: Dev Accuracy: 0.8015988364815712 Dev Loss:0.5813411459326744\n",
      "Test Loss: 0.538 | Test Acc: 80.14%\n",
      "most effective words:  ['Jungle', 'farther', 'COLLABS', 'winners', 'lemon', 'brunch', 'gabi', 'Trapped', 'volcanic', 'PinkBunnyGirl43']\n",
      "less effective words:  ['<pad>', 'Your', 'showmakers', '\\\\nInside', 'Plans', 'sneakerhead', 'http://bit.ly/2gRsJUQ\\\\n\\\\nSend', 'Wars', 'basically', 'Tang']\n"
     ]
    }
   ],
   "source": [
    "def model_selection(model_dict, model_txt=\"avg_embedding\"):\n",
    "    '''\n",
    "    Helper function for model selection\n",
    "    '''\n",
    "    return model_dict[model_txt]\n",
    "\n",
    "# Comparing the performance of different models, the original \n",
    "# version of the CAPP 30255 assignment\n",
    "def compare_models(model_dict):\n",
    "    '''\n",
    "    The function presents and compare the performances of\n",
    "    different neural network models and store the best\n",
    "    models of each model type in the output dictionary\n",
    "    \n",
    "    Inputs: model_dict: dictionary of model types used in training\n",
    "    Return: dictionary of best models of each model type\n",
    "    '''\n",
    "    best_models = {}\n",
    "    for key, value in model_dict.items():\n",
    "        print(\"currently training the model: \", key)\n",
    "\n",
    "        model = model_selection(model_dict, key)\n",
    "        model = model.to(device)\n",
    "        tm = Training_module(model)\n",
    "        best_model = tm.train_model(train_iterator, valid_iterator)\n",
    "        best_models[key] = best_model\n",
    "        \n",
    "        tm.model = best_model\n",
    "        test_loss, test_acc = tm.evaluate(test_iterator)\n",
    "        print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "        \n",
    "    return best_models\n",
    "\n",
    "# Searching for phrases with highest norm values, modify the original \n",
    "# version of the CAPP 30255 assignment\n",
    "def get_effective_norms(best_models, selected_mkey=\"avg_embedding\"):\n",
    "    '''\n",
    "    The model presents 10 most effective and 10 less effective\n",
    "    phrases used in the classification\n",
    "    Inputs: \n",
    "        best_models: dictionary of best model of each model type\n",
    "        selected_mkey: selected model type\n",
    "    '''\n",
    "    best_model = best_models[selected_mkey]\n",
    "    strong_words = []\n",
    "    weak_words = []\n",
    "    emb_weight = best_model.embedding.weight.data\n",
    "    top_indices = torch.norm(emb_weight, p=2, dim=1).detach().topk(10).indices\n",
    "    bottom_indices = torch.norm(emb_weight, p=2, dim=1).detach().topk(10, largest=False).indices\n",
    "\n",
    "    for idx in top_indices:\n",
    "        strong_words.append(TEXT.vocab.itos[idx])\n",
    "    \n",
    "    for idx in bottom_indices:\n",
    "        weak_words.append(TEXT.vocab.itos[idx])\n",
    "    \n",
    "    print(\"most effective words: \", strong_words)\n",
    "    print(\"less effective words: \", weak_words)\n",
    "\n",
    "best_models = compare_models(MODEL_DICT)\n",
    "get_effective_norms(best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "KnhR3qF_iNYc"
   ],
   "name": "mlpp20-hw2-collab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
